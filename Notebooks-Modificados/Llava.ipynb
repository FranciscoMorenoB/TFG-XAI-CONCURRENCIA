{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicación de Llava de los gráficos generados por las técnicas XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\juand\\anaconda3\\lib\\site-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\juand\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\juand\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\juand\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\juand\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\juand\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\juand\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\juand\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\juand\\anaconda3\\lib\\site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\juand\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\juand\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\juand\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\juand\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\juand\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\juand\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\juand\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\juand\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\juand\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.50.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, LlavaNextForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llava para una explicación de SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable que controla la longitud max. de la respuesta es max_new_tokens. Si la poneis =100 os va a generar la respuesta a la mitad, como la que pasé por was. Yo creo que lo óptimo sería ponerlo a 200, xq si lo pones a 300 te tarda +1000 minutos en generar una respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llava para una explicación de LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf7b718f1644955aeb3f429995f7a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "# Nombre del modelo en Hugging Face\n",
    "model_name = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "\n",
    "# Cargar el modelo y el procesador de imágenes\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", device_map=\"auto\", torch_dtype=torch.float16, low_cpu_mem_usage=True) \n",
    "\n",
    "# Imagen a analizar (gráfico)\n",
    "image = Image.open(\"./imagenes/ALE_2_Charts.png\")\n",
    "\n",
    "# Contexto personalizado para mejorar la respuesta\n",
    "context_and_question= \"In the image we have two graphs, the objective of these graphs is to understand the prediction of a machine learning classifier model on a sample. Therefore, the graph shows the most relevant input features of the model for the prediction. Each feature is represented in green if it has served favorably for the prediction of the class in question or in red if it has not helped to predict the class in question. On the x-axis of the graph we have each feature and on the y-axis its importance value. Analyzing the graph on the left, we see that the most important feature has been f3-4-c since it has a high importance value. We see that f2-4-u and f2-3-w have also helped but have not been as important, on the other hand, the feature f3-5-r has not helped the prediction of this class. Knowing this, answer the following true or false question. In the graph on the right, the most important feature is f2-12-w?\"\n",
    "prompt = f\"<image>\\nUSER: {context_and_question}\\nASSISTANT:\"\n",
    "# Procesar la imagen y el texto\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juand\\anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\juand\\anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32000, 32000,  ..., 28723, 28705,     2]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = processor(image, prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generar respuesta con opciones personalizadas\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,  # Máximo de tokens en la respuesta\n",
    "    temperature=0.1,  # Control de aleatoriedad (más bajo = más preciso)\n",
    "    top_p=0.9,  # Método de muestreo para respuestas variadas\n",
    "    repetition_penalty=1.2,  # Evita respuestas repetitivas\n",
    ")\n",
    "# Decodificar y mostrar la respuesta\n",
    "answer = processor.batch_decode(output, skip_special_tokens=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nUSER: In the image we have two graphs, the objective of these graphs is to understand the prediction of a machine learning classifier model on a sample. Analyzing the graph on the left, we see that the most important feature has been f3-4-c since it has a high importance value. We see that f2-4-u and f2-3-w have also helped but have not been as important. On the other hand, the feature f3-5-r has not contributed to the prediction of this class. Knowing this, could you perform an analysis of the right graph?\\nASSISTANT: Certainly! The right graph shows the local explanation for class A. It appears to be a heatmap representation where each row corresponds to a different instance in the dataset, and each column represents a different feature. The color intensity indicates the contribution or influence of each feature towards the classification of class A.\\n\\nFrom what I can observe, there are several features with significant contributions to the classification of class A. For example, f10-u seems to play a crucial role in distinguishing instances']\n"
     ]
    }
   ],
   "source": [
    "# Contexto personalizado para mejorar la respuesta\n",
    "context_and_question= \"In the image we have two graphs, the objective of these graphs is to understand the prediction of a machine learning classifier model on a sample. Analyzing the graph on the left, we see that the most important feature has been f3-4-c since it has a high importance value. We see that f2-4-u and f2-3-w have also helped but have not been as important. On the other hand, the feature f3-5-r has not contributed to the prediction of this class. Knowing this, could you perform an analysis of the right graph?\"\n",
    "prompt = f\"<image>\\nUSER: {context_and_question}\\nASSISTANT:\"\n",
    "\n",
    "inputs = processor(image, prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generar respuesta con opciones personalizadas\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,  # Máximo de tokens en la respuesta\n",
    "    temperature=0.1,  # Control de aleatoriedad (más bajo = más preciso)\n",
    "    top_p=0.9,  # Método de muestreo para respuestas variadas\n",
    "    repetition_penalty=1.2,  # Evita respuestas repetitivas\n",
    ")\n",
    "answer = processor.batch_decode(output, skip_special_tokens=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nUSER: In the image we have two graphs, the objective of these graphs is to understand the prediction of a machine learning classifier model on a sample. Therefore, the graph shows the most relevant input features of the model for the prediction. Each feature is represented in green if it has served favorably for the prediction of the class in question or in red if it has not helped to predict the class in question. On the x-axis of the graph we have each feature and on the y-axis its importance value. Analyzing the graph on the left, we see that the most important feature has been f3-4-c since it has a high importance value. We see that f2-4-u and f2-3-w have also helped but have not been as important, on the other hand, the feature f3-5-r has not helped the prediction of this class. Knowing this, could you tell me which is the most important feature in the right graph?\\nASSISTANT: Based on the information provided by the image, I can infer that the most important feature in the right graph is \"f10-u\" because it has a higher importance value than the others. This suggests that this particular feature played a significant role in the prediction made by the machine learning classifier model for the given class. The other features such as \"f10-d\", \"f10-v\", and \"f10-a\" are less important compared to \"f']\n"
     ]
    }
   ],
   "source": [
    "# Contexto personalizado para mejorar la respuesta\n",
    "context_and_question= \"In the image we have two graphs, the objective of these graphs is to understand the prediction of a machine learning classifier model on a sample. Therefore, the graph shows the most relevant input features of the model for the prediction. Each feature is represented in green if it has served favorably for the prediction of the class in question or in red if it has not helped to predict the class in question. On the x-axis of the graph we have each feature and on the y-axis its importance value. Analyzing the graph on the left, we see that the most important feature has been f3-4-c since it has a high importance value. We see that f2-4-u and f2-3-w have also helped but have not been as important, on the other hand, the feature f3-5-r has not helped the prediction of this class. Knowing this, could you tell me which is the most important feature in the right graph?\"\n",
    "prompt = f\"<image>\\nUSER: {context_and_question}\\nASSISTANT:\"\n",
    "\n",
    "inputs = processor(image, prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generar respuesta con opciones personalizadas\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,  # Máximo de tokens en la respuesta\n",
    "    temperature=0.1,  # Control de aleatoriedad (más bajo = más preciso)\n",
    "    top_p=0.9,  # Método de muestreo para respuestas variadas\n",
    "    repetition_penalty=1.2,  # Evita respuestas repetitivas\n",
    ")\n",
    "answer = processor.batch_decode(output, skip_special_tokens=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probamos BLIP-2 para el mismo ejemplo anterior a ver que tal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050ab9b8d8654f52912f56537243c2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\", use_fast=True)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", device_map={\"\": \"cpu\"}, torch_dtype=torch.float16)\n",
    "image = Image.open(\"./imagenes/LIME_CNN_CLASE_V.png\").convert(\"RGB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta:¿Es un grafico lo de la imagen? Respuesta:¿Es un grafico lo de la imagen?\n"
     ]
    }
   ],
   "source": [
    "context_and_question= (\n",
    "    \"Pregunta: Tenemos una imagen con una muestra y un grafico. La muestra contiene las caracteristicas de entrada al modelo de clasificacion y la clase a la que pertenece la muestra. \"  \n",
    "    \"El grafico ha sido generado por Lime, una tecnica de explicabilidad aplicada a al modelo de clasificacion.\"\n",
    "    \"Este modelo de clasificacion, clasifica las muestras en cuatro clases distintas. Lime intentar explicar porque el modelo ha predicho una clase, dando valores de \"\n",
    "    \"relevancia a las caracteristicas de entrada del modelo. Explicando asi cuales son los atributos que para el modelo son mas importantes cuando hace una prediccion para esa clase. \"\n",
    "    \"El resultado de Lime es este grafico de barras asociado a una clase. En el eje Y del grafico tenemos las caracteristicas de entrada del modelo y en eje X el valor de importancia.\"\n",
    "    \"En el eje X, la caracteristica son representadas con una tupla de tres posiciones (Funcion, posicion, operacion), \"\n",
    "    \"por ejemplo: \"\"f3-4-c\"\", \"\"f2-12-r\"\", \"\"f1-16-_, \"\"f2-0-.\"\". Donde por ejemplo \"\"f3-4-c\"\" es la caracteristica que representa la operacion \"\"c\"\", en la \"\n",
    "    \"posicion cuarta(4), de la funcion 3 (f3) Las barras del grafico seran de color verde o rojo. Verde si la caracteristica ayuda a clasificar la muestra \"\n",
    "    \"favorablemente para esa clase. Rojo si la catacteristica ayuda a discriminar la muestra como que no es pertenciente a esa clase. Sabiendo esto responde a la siguiente pregunta:\"\n",
    "    \"¿Cuales son las caracteristicas que más relevantes para la clase V, segun esta muestra? Respuesta:\"\n",
    ")\n",
    "\n",
    "pregunta_corta= (\n",
    "    \"Pregunta:¿Es un grafico lo de la imagen? Respuesta:\"\n",
    ")\n",
    "#prompt = \"Question: how many cats are there? Answer:\"\n",
    "inputs = processor(images=image, text=pregunta_corta, return_tensors=\"pt\").to(device=\"cpu\", dtype=torch.float16)\n",
    "\n",
    "generated_ids = model.generate(**inputs,\n",
    "    max_new_tokens=50,  # Máximo de tokens en la respuesta\n",
    "    #temperature=0.3,  # Control de aleatoriedad (más bajo = más preciso)\n",
    "    #top_p=0.9,  # Método de muestreo para respuestas variadas\n",
    "    #repetition_penalty=1.2,\n",
    "    #do_sample=True  # Evita respuestas repetitivas\n",
    "    )\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tenemos una imagen con una muestra y un grafico. La muestra contiene las caracteristicas de entrada al modelo de clasificacion y la clase a la que pertenece la muestra. El grafico ha sido generado por Lime, una tecnica de explicabilidad aplicada a al modelo de clasificacion.Este modelo de clasificacion, clasifica las muestras en cuatro clases distintas. Lime intentar explicar porque el modelo ha predicho una clase, dando valores de relevancia a las caracteristicas de entrada del modelo. Explicando asi cuales son los atributos que para el modelo son mas importantes cuando hace una prediccion para esa clase. El resultado de Lime es este grafico de barras asociado a una clase. En el eje Y del grafico tenemos las caracteristicas de entrada del modelo y en eje X el valor de importancia.En el eje X, la caracteristica son representadas con una tupla de tres posiciones (Funcion, posicion, operacion), por ejemplo: f3-4-c, f2-12-r, f1-16-_, f2-0-.. Donde por ejemplo f3-4-c es la caracteristica que representa la operacion c, en la posicion cuarta(4), de la funcion 3 (f3) Las barras del grafico seran de color verde o rojo. Verde si la caracteristica ayuda a clasificar la muestra favorablemente para esa clase. Rojo si la catacteristica ayuda a discriminar la muestra como que no es pertenciente a esa clase. Sabiendo esto responde a la siguiente pregunta:¿Cuales son las caracteristicas que más relevantes para la clase V, segun esta muestra?\\n']\n"
     ]
    }
   ],
   "source": [
    "answer = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llava para una explicación de ALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\franm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:3513: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  6.92it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
      "c:\\Users\\franm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\processing_auto.py:236: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# 👉 Ruta de la imagen (SHAP, LIME, ALE, etc.)\n",
    "image_path = \"./imagenes/ALE_LSTM_CARAC_W.png\"  # Cambia la ruta si hace falta\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# 👉 Token de Hugging Face\n",
    "hf_token = \"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\"  # Sustituye esto por tu token real\n",
    "\n",
    "# 👉 Cargar modelo LLaVA desde Hugging Face\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\", use_auth_token=hf_token)\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_auth_token=hf_token)\n",
    "\n",
    "# 👉 Pregunta con contexto para el gráfico\n",
    "question = \"En esta imagen se pueden ver 3 gráficas de ALE que representan el efecto acumulado de la característica 'w' en la predicción del modelo. En cada gráfico se observa el efecto de la característica en distintas posiciones de la muestra, teniendo en cuenta que hay 16 posiciones. En un gráfico aparece en la posicion 1, en otro en la posicion 8, y en otro en la posicion 13, aunque esto se puede observar en la gráfica. ¿Cómo influye la posición de 'w' en las predicciones?\" \\\n",
    "    \n",
    "# 👉 Preparar inputs\n",
    "prompt = f\"<image>\\nUSER: {question}\\nASSISTANT:\"\n",
    "inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "# 👉 Generar respuesta\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "# 👉 Mostrar respuesta\n",
    "response = processor.batch_decode(output, skip_special_tokens=True)\n",
    "print(\"🧠 Respuesta de LLaVA:\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
