{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicación de Llava de los gráficos generados por las técnicas XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting accelerate\n",
      "  Downloading accelerate-1.5.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\franm\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.29.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: networkx in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\franm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Downloading accelerate-1.5.2-py3-none-any.whl (345 kB)\n",
      "   ---------------------------------------- 0.0/345.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 345.1/345.1 kB 10.8 MB/s eta 0:00:00\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.5.2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llava para una explicación de SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# 👉 Ruta de la imagen (SHAP, LIME, ALE, etc.)\n",
    "image_path = \"./imagenes/SHAP_LSTM_CLASE_A.png\"  # Cambia la ruta si hace falta\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# 👉 Token de Hugging Face\n",
    "hf_token = \"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\"  # Sustituye esto por tu token real\n",
    "\n",
    "# 👉 Cargar modelo LLaVA desde Hugging Face\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\", use_auth_token=hf_token)\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_auth_token=hf_token)\n",
    "\n",
    "# 👉 Pregunta con contexto para el gráfico\n",
    "question = \"Este gráfico fue generado por SHAP para explicar una predicción sobre la clase 'A', que significa 'violacion de atomicidad'. En la imagen ves a que característica se corresponde cada número. ¿Qué características tienen más peso y menos peso para esta predicción?\"\n",
    "\n",
    "# 👉 Preparar inputs\n",
    "prompt = f\"<image>\\nUSER: {question}\\nASSISTANT:\"\n",
    "inputs = processor(prompt, image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 👉 Generar respuesta\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "# 👉 Mostrar respuesta\n",
    "response = processor.batch_decode(output, skip_special_tokens=True)\n",
    "print(\"🧠 Respuesta de LLaVA:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llava para una explicación de LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# 👉 Ruta de la imagen (SHAP, LIME, ALE, etc.)\n",
    "image_path = \"./imagenes/LIME_CNN_CLASE_V.png\"  # Cambia la ruta si hace falta\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# 👉 Token de Hugging Face\n",
    "hf_token = \"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\"  # Sustituye esto por tu token real\n",
    "\n",
    "# 👉 Cargar modelo LLaVA desde Hugging Face\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\", use_auth_token=hf_token)\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_auth_token=hf_token)\n",
    "\n",
    "# 👉 Pregunta con contexto para el gráfico\n",
    "question = \"Este gráfico fue generado por LIME para explicar la predicción de una muestra de clase R en el modelo CNN. ¿Qué características ayudan al modelo a predecir esta clase? Ten en cuenta que la muestra a predecir por el modelo sale en la parte superior de la imagen.\"\n",
    "\n",
    "# 👉 Preparar inputs\n",
    "prompt = f\"<image>\\nUSER: {question}\\nASSISTANT:\"\n",
    "inputs = processor(prompt, image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 👉 Generar respuesta\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "# 👉 Mostrar respuesta\n",
    "response = processor.batch_decode(output, skip_special_tokens=True)\n",
    "print(\"🧠 Respuesta de LLaVA:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llava para una explicación de ALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# 👉 Ruta de la imagen (SHAP, LIME, ALE, etc.)\n",
    "image_path = \"./imagenes/LIME_CNN_CLASE_V.png\"  # Cambia la ruta si hace falta\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# 👉 Token de Hugging Face\n",
    "hf_token = \"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\"  # Sustituye esto por tu token real\n",
    "\n",
    "# 👉 Cargar modelo LLaVA desde Hugging Face\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\", use_auth_token=hf_token)\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_auth_token=hf_token)\n",
    "\n",
    "# 👉 Pregunta con contexto para el gráfico\n",
    "question = \"En esta imagen se pueden ver 3 gráficas de ALE que representan el efecto acumulado de la característica 'w' en la predicción del modelo. En cada gráfico se observa el efecto de la característica en distintas posiciones de la muestra, teniendo en cuenta que hay 16 posiciones. En un gráfico aparece en la posicion 1, en otro en la posicion 8, y en otro en la posicion 13, aunque esto se puede observar en la gráfica. ¿Cómo influye la posición de 'w' en las predicciones?\"\n",
    "\n",
    "# 👉 Preparar inputs\n",
    "prompt = f\"<image>\\nUSER: {question}\\nASSISTANT:\"\n",
    "inputs = processor(prompt, image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 👉 Generar respuesta\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "# 👉 Mostrar respuesta\n",
    "response = processor.batch_decode(output, skip_special_tokens=True)\n",
    "print(\"🧠 Respuesta de LLaVA:\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
