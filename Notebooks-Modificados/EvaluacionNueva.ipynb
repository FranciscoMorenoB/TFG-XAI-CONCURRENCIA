{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e05a8202",
   "metadata": {},
   "source": [
    "#### Es necesario aplicar el siguiente parche tan solo una vez a one_hot_encode para que no salga un error cada vez que ejecutemos las evaluaciones. Si no se aplica el siguiente código las evaluaciones darán el siguiente error por no haber muestras de distintas clasificaciones: \n",
    "---------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "Cell In[1], line 59\n",
    "     56 separated += separated_extra  # Inyectamos muestras de relleno\n",
    "     58 # Paso 5: One-hot encoding\n",
    "---> 59 one_hotted = data.one_hot_encode(separated)\n",
    "     61 # Paso 6: Eliminar muestras de relleno\n",
    "     62 one_hotted = one_hotted[:-len(extra_samples)]\n",
    "\n",
    "File c:\\Users\\franm\\TFG-XAI-CONCURRENCIA\\Notebooks-Modificados\\generate_data.py:117, in Data.one_hot_encode(self, samples_char_sep)\n",
    "    114 # unique_chars.remove(True)\n",
    "    115 # unique_chars.remove(False)\n",
    "    116  unique_chars.remove('A')\n",
    "--> 117  unique_chars.remove('R')\n",
    "    118  unique_chars.remove('D')\n",
    "    119  unique_chars.remove('V')\n",
    "\n",
    "KeyError: 'R'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4157822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_data import Data\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "def safe_one_hot_encode(self, samples_char_sep):\n",
    "    df = pd.DataFrame(samples_char_sep, columns=self.feature_names + ['Correct'])\n",
    "    unique_chars = set()\n",
    "\n",
    "    for col in df.columns[:-1]:  # Excluye la columna de la clase\n",
    "        unique_chars = unique_chars.union(df[col].unique())\n",
    "\n",
    "    # Evita errores por KeyError si no están presentes\n",
    "    for label in ['A', 'R', 'D', 'V']:\n",
    "        unique_chars.discard(label)\n",
    "\n",
    "    unique_chars = sorted(list(unique_chars))\n",
    "\n",
    "    self.le = LabelEncoder()\n",
    "    self.le.fit(unique_chars)\n",
    "\n",
    "    self.feature_names_one_hotted = []\n",
    "    for i in range(3):\n",
    "        for j in range(self.layer_size):\n",
    "            for c in self.le.classes_:\n",
    "                self.feature_names_one_hotted.append(f'f{i + 1}-{j}-{c}')\n",
    "\n",
    "    df_no_label = df.drop(['Correct'], axis=1)\n",
    "    df_encoded = df_no_label.apply(self.le.transform)\n",
    "\n",
    "    unique_labels = set()\n",
    "    for col in df_encoded:\n",
    "        unique_labels = unique_labels.union(df_encoded[col].unique())\n",
    "    df_unique_labels = pd.DataFrame(unique_labels)\n",
    "\n",
    "    for i in range(1, 3 * self.layer_size):\n",
    "        df_unique_labels[i] = df_unique_labels[0]\n",
    "    self.unique_labels = df_unique_labels\n",
    "\n",
    "    self.ohe = OneHotEncoder(categories='auto')\n",
    "    self.ohe.fit(df_unique_labels)\n",
    "\n",
    "    one_hotted_df = pd.DataFrame(self.ohe.transform(df_encoded).toarray(), columns=self.feature_names_one_hotted)\n",
    "    self.num_one_hot_encodings = int(one_hotted_df.shape[1] / self.layer_size / 3)\n",
    "    one_hotted_df['label'] = df['Correct']\n",
    "\n",
    "    columns_one_hotted = list(one_hotted_df.columns.values[:-1])\n",
    "\n",
    "    self.f1_start = 0\n",
    "    self.f2_start = self.f1_start\n",
    "    while (columns_one_hotted[self.f2_start].find('f2') == -1):\n",
    "        self.f2_start += 1\n",
    "\n",
    "    self.f3_start = self.f2_start\n",
    "    while (columns_one_hotted[self.f3_start].find('f3') == -1):\n",
    "        self.f3_start += 1\n",
    "\n",
    "    self.f3_end = len(columns_one_hotted)\n",
    "\n",
    "    return np.array(one_hotted_df)\n",
    "\n",
    "# Aplicar el parche\n",
    "Data.one_hot_encode = safe_one_hot_encode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c36cf9a",
   "metadata": {},
   "source": [
    "#### Generamos el nuevo dataset y lo evaluamos para el modelo LSTM del 4º filtro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2964a8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Atomicity violation cases wrong predicted:\n",
      "Sample [',.,_,,_,_,._.,_', '...w.u.........', '_______r___d___'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,_', '.........w..u..', '___rd__________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,_', '..........w...u', '________r_d____'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,_', 'wu.............', '_r___d_________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,_', '.....w.u.......', '_rd____________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,_', '..........w.u..', '_________r_d___'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,_', 'wu.............', '_________r__d__'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,_', '.........wu....', '_r___d_________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,_', '..........w.u..', 'rd_____________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,_', '...w...u.......', '___r___d_______'] | Prediction V\n",
      "\n",
      "\n",
      "0 DeadLock cases wrong predicted:\n",
      "\n",
      "\n",
      "0 Data race cases wrong predicted:\n",
      "\n",
      "\n",
      "0 Valid cases wrong predicted:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\franm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\franm\\AppData\\Local\\Temp\\ipykernel_9640\\3957508601.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load('./bestmodels/best_LSTM_model_' + experiment_name, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from generate_data import Data\n",
    "from evaluate import get_wrong_predictions_bycases, print_wrong_preds_bycases\n",
    "from models import LSTM_Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paso 1: Inicializar generador de datos\n",
    "data = Data(layer_size=15, interop_distances=[0], permutation_intervals=1)\n",
    "\n",
    "# Paso 2: Generar muestras con f2='wu' y f3='rd'\n",
    "def generate_atomicity_samples_rd(n=10):\n",
    "    layer_size = 15\n",
    "    base_f1 = ',.,_,,_,_,._.,__'\n",
    "    base_f1 = base_f1[:layer_size].ljust(layer_size, '.')  # asegurar longitud\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(n):\n",
    "        f2 = ['.'] * layer_size\n",
    "        pos_w = random.randint(0, layer_size - 5)\n",
    "        pos_u = min(pos_w + 1 + random.randint(0, 3), layer_size - 1)\n",
    "        f2[pos_w] = 'w'\n",
    "        f2[pos_u] = 'u'\n",
    "        f2 = ''.join(f2)\n",
    "\n",
    "        f3 = ['_'] * layer_size\n",
    "        pos_r = random.randint(0, layer_size - 5)\n",
    "        pos_d = min(pos_r + 1 + random.randint(0, 3), layer_size - 1)\n",
    "        f3[pos_r] = 'r'\n",
    "        f3[pos_d] = 'd'\n",
    "        f3 = ''.join(f3)\n",
    "\n",
    "        samples.append([base_f1, f2, f3, 'A'])\n",
    "\n",
    "    return samples\n",
    "\n",
    "# Paso 3: Generar muestras artificiales\n",
    "custom_samples = generate_atomicity_samples_rd(10)\n",
    "separated = data.separate_string_chars(custom_samples)\n",
    "\n",
    "# Paso 4: Forzar inclusión de todos los caracteres usados en el entrenamiento\n",
    "caracteres_entrenamiento = list(\".,_wurdc\")  # He forzado que coincidan con todos los caracteres con el entrenamiento porque si no falla el modelo\n",
    "layer_size = data.layer_size\n",
    "extra_samples = []\n",
    "\n",
    "for c in caracteres_entrenamiento:\n",
    "    f1 = c * layer_size\n",
    "    f2 = c * layer_size\n",
    "    f3 = c * layer_size\n",
    "    extra_samples.append([f1, f2, f3, 'A'])\n",
    "\n",
    "separated_extra = data.separate_string_chars(extra_samples)\n",
    "separated += separated_extra  # Inyectamos muestras de relleno\n",
    "\n",
    "# Paso 5: One-hot encoding\n",
    "one_hotted = data.one_hot_encode(separated)\n",
    "\n",
    "# Paso 6: Eliminar muestras de relleno\n",
    "one_hotted = one_hotted[:-len(extra_samples)]\n",
    "\n",
    "# Paso 7: Cargar modelo LSTM entrenado\n",
    "experiment_name = \"interop_1\"\n",
    "best_lstm_model = LSTM_Model(data, 16, 32, 8).to(device)\n",
    "best_lstm_model.load_state_dict(\n",
    "    torch.load('./bestmodels/best_LSTM_model_' + experiment_name, map_location=device)\n",
    ")\n",
    "best_lstm_model.eval()\n",
    "\n",
    "# Paso 8: Preparar datos para evaluación\n",
    "x_test = data.to_lstm_format(one_hotted[:, :-1])\n",
    "y_test = torch.tensor([0] * len(custom_samples))  # 'A' = índice 0\n",
    "original_inputs = [s[:3] for s in custom_samples]\n",
    "\n",
    "# Paso 9: Evaluación\n",
    "wrong_preds = get_wrong_predictions_bycases(best_lstm_model, x_test, y_test, original_inputs)\n",
    "print_wrong_preds_bycases(wrong_preds, k=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cd4cda",
   "metadata": {},
   "source": [
    "#### Generamos el nuevo dataset y lo evaluamos para el modelo CNN del 4º filtro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "586bbd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Atomicity violation cases wrong predicted:\n",
      "Sample [',.,_,,_,_,._.,_', '.........wu....', '______r__d_____'] | Prediction R\n",
      "Sample [',.,_,,_,_,._.,_', '....w...u......', '__rd___________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,_', '........w...u..', '_______rd______'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,_', '........w.u....', '_____r_d_______'] | Prediction R\n",
      "Sample [',.,_,,_,_,._.,_', '.....w...u.....', '___r__d________'] | Prediction R\n",
      "Sample [',.,_,,_,_,._.,_', 'w..u...........', 'r_d____________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,_', '..........w.u..', 'r__d___________'] | Prediction R\n",
      "Sample [',.,_,,_,_,._.,_', '......w.u......', '__r_d__________'] | Prediction R\n",
      "Sample [',.,_,,_,_,._.,_', 'w..u...........', 'r___d__________'] | Prediction R\n",
      "Sample [',.,_,,_,_,._.,_', '.........wu....', 'rd_____________'] | Prediction V\n",
      "\n",
      "\n",
      "0 DeadLock cases wrong predicted:\n",
      "\n",
      "\n",
      "0 Data race cases wrong predicted:\n",
      "\n",
      "\n",
      "0 Valid cases wrong predicted:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\franm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\franm\\AppData\\Local\\Temp\\ipykernel_9640\\439274189.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_cnn_model.load_state_dict(torch.load('./bestmodels/best_CNN_model_' + experiment_name))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from generate_data import Data\n",
    "from evaluate import get_wrong_predictions_bycases, print_wrong_preds_bycases\n",
    "from models import CNN_Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paso 1: Inicializar generador de datos\n",
    "data = Data(layer_size=15, interop_distances=[0], permutation_intervals=1)\n",
    "\n",
    "# Paso 2: Generar muestras con f2='wu' y f3='rd'\n",
    "def generate_atomicity_samples_rd(n=10):\n",
    "    layer_size = 15\n",
    "    base_f1 = ',.,_,,_,_,._.,__'\n",
    "    base_f1 = base_f1[:layer_size].ljust(layer_size, '.')  # asegurar longitud\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(n):\n",
    "        f2 = ['.'] * layer_size\n",
    "        pos_w = random.randint(0, layer_size - 5)\n",
    "        pos_u = min(pos_w + 1 + random.randint(0, 3), layer_size - 1)\n",
    "        f2[pos_w] = 'w'\n",
    "        f2[pos_u] = 'u'\n",
    "        f2 = ''.join(f2)\n",
    "\n",
    "        f3 = ['_'] * layer_size\n",
    "        pos_r = random.randint(0, layer_size - 5)\n",
    "        pos_d = min(pos_r + 1 + random.randint(0, 3), layer_size - 1)\n",
    "        f3[pos_r] = 'r'\n",
    "        f3[pos_d] = 'd'\n",
    "        f3 = ''.join(f3)\n",
    "\n",
    "        samples.append([base_f1, f2, f3, 'A'])\n",
    "\n",
    "    return samples\n",
    "\n",
    "# Paso 3: Generar muestras artificiales\n",
    "custom_samples = generate_atomicity_samples_rd(10)\n",
    "separated = data.separate_string_chars(custom_samples)\n",
    "\n",
    "# Paso 4: Forzar inclusión de todos los caracteres usados en el entrenamiento\n",
    "caracteres_entrenamiento = list(\".,_wurdc\")  # He forzado que coincidan con todos los caracteres con el entrenamiento porque si no falla el modelo\n",
    "layer_size = data.layer_size\n",
    "extra_samples = []\n",
    "\n",
    "for c in caracteres_entrenamiento:\n",
    "    f1 = c * layer_size\n",
    "    f2 = c * layer_size\n",
    "    f3 = c * layer_size\n",
    "    extra_samples.append([f1, f2, f3, 'A'])\n",
    "\n",
    "separated_extra = data.separate_string_chars(extra_samples)\n",
    "separated += separated_extra  # Inyectamos muestras de relleno\n",
    "\n",
    "# Paso 5: One-hot encoding\n",
    "one_hotted = data.one_hot_encode(separated)\n",
    "\n",
    "# Paso 6: Eliminar muestras de relleno\n",
    "one_hotted = one_hotted[:-len(extra_samples)]\n",
    "\n",
    "# Paso 7: Cargar modelo LSTM entrenado\n",
    "experiment_name = \"interop_1\"\n",
    "best_cnn_model = CNN_Model(data, 32, 64, 4, -1,).to(device)\n",
    "best_cnn_model.load_state_dict(torch.load('./bestmodels/best_CNN_model_' + experiment_name))\n",
    "best_cnn_model.eval()\n",
    "\n",
    "# Paso 8: Preparar datos para evaluación\n",
    "x_test = data.to_conv_format(one_hotted[:, :-1])\n",
    "y_test = torch.tensor([0] * len(custom_samples))  # 'A' = índice 0\n",
    "original_inputs = [s[:3] for s in custom_samples]\n",
    "\n",
    "# Paso 9: Evaluación\n",
    "wrong_preds = get_wrong_predictions_bycases(best_cnn_model, x_test, y_test, original_inputs)\n",
    "print_wrong_preds_bycases(wrong_preds, k=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5342987f",
   "metadata": {},
   "source": [
    "#### Generamos el nuevo dataset y lo evaluamos para el modelo deepset del 4º filtro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef976599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Atomicity violation cases wrong predicted:\n",
      "Sample [',.,_,,_,_,._.,__', '...........wu...', '___r___d________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', '......w..u......', '_______r_d______'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', '.w...u..........', '________rd______'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', '.....w..u.......', '_____r___d______'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', 'w...u...........', '___________r___d'] | Prediction R\n",
      "Sample [',.,_,,_,_,._.,__', '...w..u.........', '_r_d____________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', '.......w.u......', '___r___d________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', '........wu......', 'r_d_____________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', '.......wu.......', '_______r_d______'] | Prediction V\n",
      "\n",
      "\n",
      "0 DeadLock cases wrong predicted:\n",
      "\n",
      "\n",
      "0 Data race cases wrong predicted:\n",
      "\n",
      "\n",
      "0 Valid cases wrong predicted:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\franm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\franm\\AppData\\Local\\Temp\\ipykernel_9640\\4114018446.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_deepset_model.load_state_dict(torch.load('./bestmodels/best_DEEPSET_model_' + experiment_name, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from generate_data import Data\n",
    "from evaluate import get_wrong_predictions_bycases, print_wrong_preds_bycases\n",
    "from models import DEEPSET_Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paso 1: Inicializar generador de datos con layer_size = 16\n",
    "# IMPORTANTE: El modelo DEEPSET fue entrenado con layer_size = 16, no 15\n",
    "# Para que las dimensiones coincidan, debes usar 16 aquí también\n",
    "data = Data(layer_size=16, interop_distances=[0], permutation_intervals=1)\n",
    "\n",
    "# Paso 2: Generar muestras con f2='wu' y f3='rd'\n",
    "def generate_atomicity_samples_rd(n=10):\n",
    "    layer_size = data.layer_size\n",
    "    base_f1 = ',.,_,,_,_,._.,__'\n",
    "    base_f1 = base_f1[:layer_size].ljust(layer_size, '.')  # asegurar longitud\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(n):\n",
    "        f2 = ['.'] * layer_size\n",
    "        pos_w = random.randint(0, layer_size - 5)\n",
    "        pos_u = min(pos_w + 1 + random.randint(0, 3), layer_size - 1)\n",
    "        f2[pos_w] = 'w'\n",
    "        f2[pos_u] = 'u'\n",
    "        f2 = ''.join(f2)\n",
    "\n",
    "        f3 = ['_'] * layer_size\n",
    "        pos_r = random.randint(0, layer_size - 5)\n",
    "        pos_d = min(pos_r + 1 + random.randint(0, 3), layer_size - 1)\n",
    "        f3[pos_r] = 'r'\n",
    "        f3[pos_d] = 'd'\n",
    "        f3 = ''.join(f3)\n",
    "\n",
    "        samples.append([base_f1, f2, f3, 'A'])\n",
    "\n",
    "    return samples\n",
    "\n",
    "# Paso 3: Generar muestras artificiales\n",
    "custom_samples = generate_atomicity_samples_rd(10)\n",
    "separated = data.separate_string_chars(custom_samples)\n",
    "\n",
    "# Paso 4: Forzar inclusión de todos los caracteres usados en entrenamiento\n",
    "caracteres_entrenamiento = list(\".,_wurdc\")\n",
    "extra_samples = []\n",
    "\n",
    "for c in caracteres_entrenamiento:\n",
    "    f1 = c * data.layer_size\n",
    "    f2 = c * data.layer_size\n",
    "    f3 = c * data.layer_size\n",
    "    extra_samples.append([f1, f2, f3, 'A'])\n",
    "\n",
    "separated_extra = data.separate_string_chars(extra_samples)\n",
    "separated += separated_extra\n",
    "\n",
    "# Paso 5: One-hot encoding\n",
    "one_hotted = data.one_hot_encode(separated)\n",
    "\n",
    "# Paso 6: Eliminar muestras de relleno\n",
    "one_hotted = one_hotted[:-len(extra_samples)]\n",
    "\n",
    "# Paso 7: Cargar modelo DEEPSET entrenado con 128 de entrada\n",
    "experiment_name = \"interop_1\"\n",
    "best_deepset_model = DEEPSET_Model(data).to(device)\n",
    "best_deepset_model.load_state_dict(torch.load('./bestmodels/best_DEEPSET_model_' + experiment_name, map_location=device))\n",
    "best_deepset_model.eval()\n",
    "\n",
    "# Paso 8: Preparar datos para evaluación\n",
    "x_test = data.to_conv_format(one_hotted[:, :-1])  # DEEPSET también usa este formato\n",
    "y_test = torch.tensor([0] * len(custom_samples))  # Clase A = índice 0\n",
    "original_inputs = [s[:3] for s in custom_samples]\n",
    "\n",
    "# Paso 9: Evaluación\n",
    "wrong_preds = get_wrong_predictions_bycases(best_deepset_model, x_test, y_test, original_inputs)\n",
    "print_wrong_preds_bycases(wrong_preds, k=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00ef750",
   "metadata": {},
   "source": [
    "#### Generamos el nuevo dataset y lo evaluamos para el modelo deepsetV2 del 4º filtro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70702a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Atomicity violation cases wrong predicted:\n",
      "Sample [',.,_,,_,_,._.,__', '......w..u......', '____r__d________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', '.........w...u..', '_____r_d________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', '...w..u.........', '___________r__d_'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', '.......w.u......', '____r__d________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', '.........w..u...', 'r__d____________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', 'w...u...........', '_____rd_________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', '........w.u.....', 'r_d_____________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', '.....w...u......', '________rd______'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', '........wu......', '________rd______'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', 'w..u............', '________r_d_____'] | Prediction V\n",
      "\n",
      "\n",
      "0 DeadLock cases wrong predicted:\n",
      "\n",
      "\n",
      "0 Data race cases wrong predicted:\n",
      "\n",
      "\n",
      "0 Valid cases wrong predicted:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\franm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\franm\\AppData\\Local\\Temp\\ipykernel_9640\\4099876642.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_deepsetv2_model.load_state_dict(torch.load('./bestmodels/best_DEEPSETV2_model_' + experiment_name))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from generate_data import Data\n",
    "from evaluate import get_wrong_predictions_bycases, print_wrong_preds_bycases\n",
    "from models import DEEPSETV2_Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paso 1: Inicializar generador de datos con layer_size = 16\n",
    "# IMPORTANTE: El modelo DEEPSETv2 fue entrenado con layer_size = 16, no 15\n",
    "# Para que las dimensiones coincidan, debes usar 16 aquí también\n",
    "data = Data(layer_size=16, interop_distances=[0], permutation_intervals=1)\n",
    "\n",
    "# Paso 2: Generar muestras con f2='wu' y f3='rd'\n",
    "def generate_atomicity_samples_rd(n=10):\n",
    "    layer_size = data.layer_size\n",
    "    base_f1 = ',.,_,,_,_,._.,__'\n",
    "    base_f1 = base_f1[:layer_size].ljust(layer_size, '.')  # asegurar longitud\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(n):\n",
    "        f2 = ['.'] * layer_size\n",
    "        pos_w = random.randint(0, layer_size - 5)\n",
    "        pos_u = min(pos_w + 1 + random.randint(0, 3), layer_size - 1)\n",
    "        f2[pos_w] = 'w'\n",
    "        f2[pos_u] = 'u'\n",
    "        f2 = ''.join(f2)\n",
    "\n",
    "        f3 = ['_'] * layer_size\n",
    "        pos_r = random.randint(0, layer_size - 5)\n",
    "        pos_d = min(pos_r + 1 + random.randint(0, 3), layer_size - 1)\n",
    "        f3[pos_r] = 'r'\n",
    "        f3[pos_d] = 'd'\n",
    "        f3 = ''.join(f3)\n",
    "\n",
    "        samples.append([base_f1, f2, f3, 'A'])\n",
    "\n",
    "    return samples\n",
    "\n",
    "# Paso 3: Generar muestras artificiales\n",
    "custom_samples = generate_atomicity_samples_rd(10)\n",
    "separated = data.separate_string_chars(custom_samples)\n",
    "\n",
    "# Paso 4: Forzar inclusión de todos los caracteres usados en entrenamiento\n",
    "caracteres_entrenamiento = list(\".,_wurdc\")\n",
    "extra_samples = []\n",
    "\n",
    "for c in caracteres_entrenamiento:\n",
    "    f1 = c * data.layer_size\n",
    "    f2 = c * data.layer_size\n",
    "    f3 = c * data.layer_size\n",
    "    extra_samples.append([f1, f2, f3, 'A'])\n",
    "\n",
    "separated_extra = data.separate_string_chars(extra_samples)\n",
    "separated += separated_extra\n",
    "\n",
    "# Paso 5: One-hot encoding\n",
    "one_hotted = data.one_hot_encode(separated)\n",
    "\n",
    "# Paso 6: Eliminar muestras de relleno\n",
    "one_hotted = one_hotted[:-len(extra_samples)]\n",
    "\n",
    "# Paso 7: Cargar modelo DEEPSET entrenado con 128 de entrada\n",
    "experiment_name = \"interop_1\"\n",
    "best_deepsetv2_model = DEEPSETV2_Model(data).to(device)\n",
    "best_deepsetv2_model.load_state_dict(torch.load('./bestmodels/best_DEEPSETV2_model_' + experiment_name))\n",
    "best_deepsetv2_model.eval()\n",
    "\n",
    "# Paso 8: Preparar datos para evaluación\n",
    "x_test = data.to_conv_format(one_hotted[:, :-1])  # DEEPSET también usa este formato\n",
    "y_test = torch.tensor([0] * len(custom_samples))  # Clase A = índice 0\n",
    "original_inputs = [s[:3] for s in custom_samples]\n",
    "\n",
    "# Paso 9: Evaluación\n",
    "wrong_preds = get_wrong_predictions_bycases(best_deepsetv2_model, x_test, y_test, original_inputs)\n",
    "print_wrong_preds_bycases(wrong_preds, k=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e2ae22",
   "metadata": {},
   "source": [
    "#### Generamos el nuevo dataset y lo evaluamos para el modelo feedforward del 4º filtro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1aef4653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Atomicity violation cases wrong predicted:\n",
      "Sample [',.,_,,_,_,._.,__', '.w..u...........', '_______r_d______'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', '....w.u.........', '__________r___d_'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', '.......wu.......', '_________r___d__'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', 'w...u...........', '_r_d____________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', '.........w..u...', '________rd______'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', '......w.u.......', 'r_d_____________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', '.w.u............', '_r_d____________'] | Prediction V\n",
      "Sample [',.,_,,_,_,._.,__', '.....w...u......', '__________rd____'] | Prediction V\n",
      "\n",
      "\n",
      "0 DeadLock cases wrong predicted:\n",
      "\n",
      "\n",
      "0 Data race cases wrong predicted:\n",
      "\n",
      "\n",
      "0 Valid cases wrong predicted:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\franm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\franm\\AppData\\Local\\Temp\\ipykernel_9640\\2299266133.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_feedforward_model.load_state_dict(torch.load('./bestmodels/best_FEEDFORWARD_model_' + experiment_name))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from generate_data import Data\n",
    "from evaluate import get_wrong_predictions_bycases, print_wrong_preds_bycases\n",
    "from models import FEEDFORWARD_Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paso 1: Inicializar generador de datos con layer_size = 16\n",
    "# IMPORTANTE: El modelo DEEPSETv2 fue entrenado con layer_size = 16, no 15\n",
    "# Para que las dimensiones coincidan, debes usar 16 aquí también\n",
    "data = Data(layer_size=16, interop_distances=[0], permutation_intervals=1)\n",
    "\n",
    "# Paso 2: Generar muestras con f2='wu' y f3='rd'\n",
    "def generate_atomicity_samples_rd(n=10):\n",
    "    layer_size = data.layer_size\n",
    "    base_f1 = ',.,_,,_,_,._.,__'\n",
    "    base_f1 = base_f1[:layer_size].ljust(layer_size, '.')  # asegurar longitud\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(n):\n",
    "        f2 = ['.'] * layer_size\n",
    "        pos_w = random.randint(0, layer_size - 5)\n",
    "        pos_u = min(pos_w + 1 + random.randint(0, 3), layer_size - 1)\n",
    "        f2[pos_w] = 'w'\n",
    "        f2[pos_u] = 'u'\n",
    "        f2 = ''.join(f2)\n",
    "\n",
    "        f3 = ['_'] * layer_size\n",
    "        pos_r = random.randint(0, layer_size - 5)\n",
    "        pos_d = min(pos_r + 1 + random.randint(0, 3), layer_size - 1)\n",
    "        f3[pos_r] = 'r'\n",
    "        f3[pos_d] = 'd'\n",
    "        f3 = ''.join(f3)\n",
    "\n",
    "        samples.append([base_f1, f2, f3, 'A'])\n",
    "\n",
    "    return samples\n",
    "\n",
    "# Paso 3: Generar muestras artificiales\n",
    "custom_samples = generate_atomicity_samples_rd(10)\n",
    "separated = data.separate_string_chars(custom_samples)\n",
    "\n",
    "# Paso 4: Forzar inclusión de todos los caracteres usados en entrenamiento\n",
    "caracteres_entrenamiento = list(\".,_wurdc\")\n",
    "extra_samples = []\n",
    "\n",
    "for c in caracteres_entrenamiento:\n",
    "    f1 = c * data.layer_size\n",
    "    f2 = c * data.layer_size\n",
    "    f3 = c * data.layer_size\n",
    "    extra_samples.append([f1, f2, f3, 'A'])\n",
    "\n",
    "separated_extra = data.separate_string_chars(extra_samples)\n",
    "separated += separated_extra\n",
    "\n",
    "# Paso 5: One-hot encoding\n",
    "one_hotted = data.one_hot_encode(separated)\n",
    "\n",
    "# Paso 6: Eliminar muestras de relleno\n",
    "one_hotted = one_hotted[:-len(extra_samples)]\n",
    "\n",
    "# Paso 7: Cargar modelo DEEPSET entrenado con 128 de entrada\n",
    "experiment_name = \"interop_1\"\n",
    "best_feedforward_model = FEEDFORWARD_Model(data).to(device)\n",
    "best_feedforward_model.load_state_dict(torch.load('./bestmodels/best_FEEDFORWARD_model_' + experiment_name))\n",
    "best_feedforward_model.eval()\n",
    "\n",
    "# Paso 8: Preparar datos para evaluación\n",
    "x_test = data.to_conv_format(one_hotted[:, :-1])  # DEEPSET también usa este formato\n",
    "y_test = torch.tensor([0] * len(custom_samples))  # Clase A = índice 0\n",
    "original_inputs = [s[:3] for s in custom_samples]\n",
    "\n",
    "# Paso 9: Evaluación\n",
    "wrong_preds = get_wrong_predictions_bycases(best_feedforward_model, x_test, y_test, original_inputs)\n",
    "print_wrong_preds_bycases(wrong_preds, k=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
